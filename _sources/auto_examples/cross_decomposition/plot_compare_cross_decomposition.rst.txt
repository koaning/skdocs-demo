
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/cross_decomposition/plot_compare_cross_decomposition.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py:


===================================
Compare cross decomposition methods
===================================

Simple usage of various cross decomposition algorithms:

- PLSCanonical
- PLSRegression, with multivariate response, a.k.a. PLS2
- PLSRegression, with univariate response, a.k.a. PLS1
- CCA

Given 2 multivariate covarying two-dimensional datasets, X, and Y,
PLS extracts the 'directions of covariance', i.e. the components of each
datasets that explain the most shared variance between both datasets.
This is apparent on the **scatterplot matrix** display: components 1 in
dataset X and dataset Y are maximally correlated (points lie around the
first diagonal). This is also true for components 2 in both dataset,
however, the correlation across datasets for different components is
weak: the point cloud is very spherical.

.. GENERATED FROM PYTHON SOURCE LINES 23-27

.. code-block:: Python


    # Authors: The scikit-learn developers
    # SPDX-License-Identifier: BSD-3-Clause








.. GENERATED FROM PYTHON SOURCE LINES 28-30

Dataset based latent variables model
------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 30-53

.. code-block:: Python


    import numpy as np

    from sklearn.model_selection import train_test_split

    rng = np.random.default_rng(42)

    n = 500
    # 2 latents vars:
    l1 = rng.normal(size=n)
    l2 = rng.normal(size=n)

    latents = np.array([l1, l1, l2, l2]).T
    X = latents + rng.normal(size=(n, 4))
    Y = latents + rng.normal(size=(n, 4))

    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, shuffle=False)

    print("Corr(X)")
    print(np.round(np.corrcoef(X.T), 2))
    print("Corr(Y)")
    print(np.round(np.corrcoef(Y.T), 2))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Corr(X)
    [[ 1.    0.48 -0.01  0.  ]
     [ 0.48  1.   -0.01  0.  ]
     [-0.01 -0.01  1.    0.49]
     [ 0.    0.    0.49  1.  ]]
    Corr(Y)
    [[ 1.    0.47  0.03  0.05]
     [ 0.47  1.    0.03 -0.01]
     [ 0.03  0.03  1.    0.5 ]
     [ 0.05 -0.01  0.5   1.  ]]




.. GENERATED FROM PYTHON SOURCE LINES 54-59

Canonical (symmetric) PLS
-------------------------

Transform data
~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 59-67

.. code-block:: Python


    from sklearn.cross_decomposition import PLSCanonical

    plsca = PLSCanonical(n_components=2)
    plsca.fit(X_train, Y_train)
    X_train_r, Y_train_r = plsca.transform(X_train, Y_train)
    X_test_r, Y_test_r = plsca.transform(X_test, Y_test)








.. GENERATED FROM PYTHON SOURCE LINES 68-70

Scatter plot of scores
~~~~~~~~~~~~~~~~~~~~~~

.. GENERATED FROM PYTHON SOURCE LINES 70-129

.. code-block:: Python


    import matplotlib.pyplot as plt

    # On diagonal plot X vs Y scores on each components
    plt.figure(figsize=(12, 8))
    plt.subplot(221)
    plt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label="train", marker="o", s=25)
    plt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label="test", marker="o", s=25)
    plt.xlabel("x scores")
    plt.ylabel("y scores")
    plt.title(
        "Comp. 1: X vs Y (test corr = %.2f)"
        % np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1]
    )
    plt.xticks(())
    plt.yticks(())
    plt.legend(loc="best")

    plt.subplot(224)
    plt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label="train", marker="o", s=25)
    plt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label="test", marker="o", s=25)
    plt.xlabel("x scores")
    plt.ylabel("y scores")
    plt.title(
        "Comp. 2: X vs Y (test corr = %.2f)"
        % np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1]
    )
    plt.xticks(())
    plt.yticks(())
    plt.legend(loc="best")

    # Off diagonal plot components 1 vs 2 for X and Y
    plt.subplot(222)
    plt.scatter(X_train_r[:, 0], X_train_r[:, 1], label="train", marker="*", s=50)
    plt.scatter(X_test_r[:, 0], X_test_r[:, 1], label="test", marker="*", s=50)
    plt.xlabel("X comp. 1")
    plt.ylabel("X comp. 2")
    plt.title(
        "X comp. 1 vs X comp. 2 (test corr = %.2f)"
        % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1]
    )
    plt.legend(loc="best")
    plt.xticks(())
    plt.yticks(())

    plt.subplot(223)
    plt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label="train", marker="*", s=50)
    plt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label="test", marker="*", s=50)
    plt.xlabel("Y comp. 1")
    plt.ylabel("Y comp. 2")
    plt.title(
        "Y comp. 1 vs Y comp. 2 , (test corr = %.2f)"
        % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1]
    )
    plt.legend(loc="best")
    plt.xticks(())
    plt.yticks(())
    plt.show()




.. image-sg:: /auto_examples/cross_decomposition/images/sphx_glr_plot_compare_cross_decomposition_001.png
   :alt: Comp. 1: X vs Y (test corr = 0.61), Comp. 2: X vs Y (test corr = 0.66), X comp. 1 vs X comp. 2 (test corr = 0.06), Y comp. 1 vs Y comp. 2 , (test corr = -0.01)
   :srcset: /auto_examples/cross_decomposition/images/sphx_glr_plot_compare_cross_decomposition_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 130-132

PLS regression, with multivariate response, a.k.a. PLS2
-------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 132-152

.. code-block:: Python


    from sklearn.cross_decomposition import PLSRegression

    n = 1000
    q = 3
    p = 10
    X = rng.normal(size=(n, p))
    B = np.array([[1, 2] + [0] * (p - 2)] * q).T
    # each Yj = 1*X1 + 2*X2 + noize
    Y = np.dot(X, B) + rng.normal(size=(n, q)) + 5

    pls2 = PLSRegression(n_components=3)
    pls2.fit(X, Y)
    print("True B (such that: Y = XB + Err)")
    print(B)
    # compare pls2.coef_ with B
    print("Estimated B")
    print(np.round(pls2.coef_, 1))
    pls2.predict(X)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    True B (such that: Y = XB + Err)
    [[1 1 1]
     [2 2 2]
     [0 0 0]
     [0 0 0]
     [0 0 0]
     [0 0 0]
     [0 0 0]
     [0 0 0]
     [0 0 0]
     [0 0 0]]
    Estimated B
    [[ 1.   2.   0.  -0.   0.   0.   0.   0.   0.  -0. ]
     [ 1.   2.   0.  -0.  -0.  -0.  -0.  -0.1 -0.   0. ]
     [ 1.   2.   0.  -0.   0.   0.  -0.  -0.   0.   0. ]]

    array([[4.09928294, 4.27252412, 4.116446  ],
           [3.22383315, 3.36186659, 3.2829478 ],
           [6.40665836, 6.45699286, 6.28414926],
           ...,
           [1.50716084, 1.50460976, 1.5177967 ],
           [6.67188307, 6.51139993, 6.47838503],
           [5.93803911, 5.99272896, 5.91191611]], shape=(1000, 3))



.. GENERATED FROM PYTHON SOURCE LINES 153-155

PLS regression, with univariate response, a.k.a. PLS1
-----------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 155-166

.. code-block:: Python


    n = 1000
    p = 10
    X = rng.normal(size=(n, p))
    y = X[:, 0] + 2 * X[:, 1] + rng.normal(size=n) + 5
    pls1 = PLSRegression(n_components=3)
    pls1.fit(X, y)
    # note that the number of components exceeds 1 (the dimension of y)
    print("Estimated betas")
    print(np.round(pls1.coef_, 1))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Estimated betas
    [[ 1.   2.  -0.   0.  -0.   0.  -0.1  0.  -0.  -0. ]]




.. GENERATED FROM PYTHON SOURCE LINES 167-169

CCA (PLS mode B with symmetric deflation)
-----------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 169-176

.. code-block:: Python


    from sklearn.cross_decomposition import CCA

    cca = CCA(n_components=2)
    cca.fit(X_train, Y_train)
    X_train_r, Y_train_r = cca.transform(X_train, Y_train)
    X_test_r, Y_test_r = cca.transform(X_test, Y_test)








.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 0.055 seconds)


.. _sphx_glr_download_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/scikit-learn/scikit-learn/main?urlpath=lab/tree/notebooks/auto_examples/cross_decomposition/plot_compare_cross_decomposition.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_compare_cross_decomposition.ipynb <plot_compare_cross_decomposition.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_compare_cross_decomposition.py <plot_compare_cross_decomposition.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_compare_cross_decomposition.zip <plot_compare_cross_decomposition.zip>`


.. include:: plot_compare_cross_decomposition.recommendations


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
